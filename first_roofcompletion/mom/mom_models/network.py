import math
import torch
from inspect import isfunction
from functools import partial
import numpy as np
from tqdm import tqdm
import os
import sys
import json
from core.base_network import BaseNetwork

class Network(BaseNetwork):
    def __init__(self, unetMom, beta_schedule, module_name='mom_modules', **kwargs):
        super(Network, self).__init__(**kwargs)
        
        # æ ¹æ®module_nameå¯¼å…¥å¯¹åº”çš„UNet
        if module_name == 'sr3':
            from .sr3_modules.unet import UNet
        elif module_name == 'guided_diffusion':
            from .guided_diffusion_modules.unet import UNet
        elif module_name == 'mom_modules':
            from .mom_moduls.unet import UNet
        else:
            # é»˜è®¤ä½¿ç”¨mom_modules
            from .mom_moduls.unet import UNet
        
        # åªåˆå§‹åŒ–MOMæ¨¡å—
        self.MOM = UNet(**unetMom)
        self.beta_schedule = beta_schedule
        
        # é¢„è®­ç»ƒæ¨¡å‹å°†åœ¨load_pretrained_modelsæ–¹æ³•ä¸­åˆå§‹åŒ–
        self.Roofdenoise_fn = None
        self.RoofLinedenoise_fn = None
        
        # æ·»åŠ å‚æ•°åŒ–æ–¹å¼ï¼Œé»˜è®¤ä¸ºå™ªå£°é¢„æµ‹
        self.parameterization = kwargs.get('parameterization', 'eps')
        self.parameterization='v'

    def load_pretrained_models(self, roof_path, roofline_path):
        """
        åŠ è½½é¢„è®­ç»ƒçš„roofå’Œrooflineæ¨¡å‹
        
        Args:
            roof_path (str): roofæ¨¡å‹çš„è·¯å¾„
            roofline_path (str): rooflineæ¨¡å‹çš„è·¯å¾„
        """
        print("="*60)
        print("å¼€å§‹åŠ è½½é¢„è®­ç»ƒæ¨¡å‹...")
        print("="*60)
        
        roof_success = False
        roofline_success = False
        
        # åŠ è½½roofæ¨¡å‹
        if roof_path and os.path.exists(roof_path):
            print(f"æ­£åœ¨åŠ è½½Roofæ¨¡å‹: {roof_path}")
            roof_success = self._load_roof_model(roof_path)
        else:
            print(f"âŒ Roofæ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {roof_path}")
            
        # åŠ è½½rooflineæ¨¡å‹  
        if roofline_path and os.path.exists(roofline_path):
            print(f"æ­£åœ¨åŠ è½½Rooflineæ¨¡å‹: {roofline_path}")
            roofline_success = self._load_roofline_model(roofline_path)
        else:
            print(f"âŒ Rooflineæ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {roofline_path}")
        
        # å°†é¢„è®­ç»ƒæ¨¡å‹ç§»åŠ¨åˆ°GPUï¼ˆå¦‚æœCUDAå¯ç”¨ï¼‰
        if torch.cuda.is_available():
            if hasattr(self, 'Roofdenoise_fn') and self.Roofdenoise_fn is not None:
                self.Roofdenoise_fn = self.Roofdenoise_fn.cuda()
                print(f"âœ… Roofæ¨¡å‹å·²ç§»åŠ¨åˆ°GPU: {next(self.Roofdenoise_fn.parameters()).device}")
            
            if hasattr(self, 'RoofLinedenoise_fn') and self.RoofLinedenoise_fn is not None:
                self.RoofLinedenoise_fn = self.RoofLinedenoise_fn.cuda()
                print(f"âœ… Rooflineæ¨¡å‹å·²ç§»åŠ¨åˆ°GPU: {next(self.RoofLinedenoise_fn.parameters()).device}")
        
        # æ£€æŸ¥åŠ è½½ç»“æœå¹¶æ–­è¨€
        print("\n" + "="*60)
        print("é¢„è®­ç»ƒæ¨¡å‹åŠ è½½ç»“æœæ£€æŸ¥")
        print("="*60)
        print(f"Roofæ¨¡å‹åŠ è½½: {'âœ… æˆåŠŸ' if roof_success else 'âŒ å¤±è´¥'}")
        print(f"Rooflineæ¨¡å‹åŠ è½½: {'âœ… æˆåŠŸ' if roofline_success else 'âŒ å¤±è´¥'}")
        
        # æ–­è¨€æ£€æŸ¥ - å¦‚æœä»»ä½•ä¸€ä¸ªæ¨¡å‹åŠ è½½å¤±è´¥ï¼Œç¨‹åºé€€å‡º
        assert roof_success, f"âŒ Roofæ¨¡å‹åŠ è½½å¤±è´¥ï¼è¯·æ£€æŸ¥æ¨¡å‹è·¯å¾„å’Œé…ç½®: {roof_path}"
        assert roofline_success, f"âŒ Rooflineæ¨¡å‹åŠ è½½å¤±è´¥ï¼è¯·æ£€æŸ¥æ¨¡å‹è·¯å¾„å’Œé…ç½®: {roofline_path}"
        
        # éªŒè¯æ¨¡å‹æ˜¯å¦æ­£ç¡®åŠ è½½åˆ°å†…å­˜
        assert self.Roofdenoise_fn is not None, "âŒ Roofæ¨¡å‹æœªæ­£ç¡®åˆå§‹åŒ–åˆ°self.Roofdenoise_fn"
        assert self.RoofLinedenoise_fn is not None, "âŒ Rooflineæ¨¡å‹æœªæ­£ç¡®åˆå§‹åŒ–åˆ°self.RoofLinedenoise_fn"
        
        print("ğŸ‰ æ‰€æœ‰é¢„è®­ç»ƒæ¨¡å‹åŠ è½½æˆåŠŸï¼")
        print("="*60)
        
        # æ‰“å°å‚æ•°ç»Ÿè®¡
        self._print_trainable_parameters()
    
    def _load_roof_model(self, roof_path):
        """åŠ è½½roofæ¨¡å‹"""
        try:
            # è·å–roofç›®å½•è·¯å¾„
            roof_dir = os.path.dirname(roof_path)
            config_path = os.path.join(roof_dir, 'config.json')
            
            if not os.path.exists(config_path):
                print(f"âŒ Roofé…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_path}")
                return False
                
            # è¯»å–é…ç½®æ–‡ä»¶
            with open(config_path, 'r') as f:
                roof_config = json.load(f)
            
            # è·å–ç½‘ç»œé…ç½®
            network_config = roof_config['model']['which_networks'][0]['args']
            unet_config = network_config['unet']
            module_name = network_config.get('module_name', 'roof_modules')
            
            # ä¿å­˜å½“å‰çš„sys.pathçŠ¶æ€
            original_path = sys.path.copy()
            
            try:
                # å°†roofç›¸å…³è·¯å¾„æ·»åŠ åˆ°æœ€å‰é¢
                roof_models_path = os.path.join(roof_dir, 'models')
                sys.path.insert(0, roof_models_path)
                sys.path.insert(0, roof_dir)
                
                print(f"ğŸ”§ è®¾ç½®å¯¼å…¥è·¯å¾„: {roof_models_path}")
                
                # ä½¿ç”¨ä¼ ç»Ÿå¯¼å…¥æ–¹å¼
                if module_name == 'roof_modules':
                    from roof_modules.unet import UNet as RoofUNet
                elif module_name == 'guided_diffusion_modules':
                    from guided_diffusion_modules.unet import UNet as RoofUNet
                else:
                    # é»˜è®¤å°è¯•roof_modules
                    from roof_modules.unet import UNet as RoofUNet
                    
                print(f"âœ… æˆåŠŸå¯¼å…¥UNetæ¨¡å—: {module_name}")
                
            finally:
                # æ¢å¤åŸå§‹çš„sys.path
                sys.path = original_path
            
            # åˆ›å»ºroofæ¨¡å‹
            self.Roofdenoise_fn = RoofUNet(**unet_config)
            print(f"âœ… æˆåŠŸåˆ›å»ºRoofæ¨¡å‹")
            
            # åŠ è½½é¢„è®­ç»ƒæƒé‡
            checkpoint = torch.load(roof_path, map_location='cpu')
            if 'model' in checkpoint:
                state_dict = checkpoint['model']
            elif 'state_dict' in checkpoint:
                state_dict = checkpoint['state_dict']
            else:
                state_dict = checkpoint
                
            # è¿‡æ»¤æƒé‡
            filtered_dict = {}
            for k, v in state_dict.items():
                if k.startswith('denoise_fn.'):
                    new_key = k.replace('denoise_fn.', '')
                    filtered_dict[new_key] = v
                elif 'denoise_fn' not in k:
                    filtered_dict[k] = v
                    
            # åŠ è½½æƒé‡ï¼Œä½¿ç”¨strict=Falseå¿½ç•¥ä¸åŒ¹é…çš„å‚æ•°
            missing_keys, unexpected_keys = self.Roofdenoise_fn.load_state_dict(filtered_dict, strict=False)
            
            # è®¡ç®—åŠ è½½æˆåŠŸç‡
            total_params = len(self.Roofdenoise_fn.state_dict())
            loaded_params = total_params - len(missing_keys)
            success_rate = loaded_params / total_params if total_params > 0 else 0
            
            print(f"âœ… Roofæƒé‡åŠ è½½å®Œæˆ: {loaded_params}/{total_params} å‚æ•° ({success_rate*100:.1f}%)")
            if missing_keys:
                print(f"âš ï¸  ç¼ºå¤±å‚æ•°: {len(missing_keys)} ä¸ª")
            if unexpected_keys:
                print(f"âš ï¸  å¤šä½™å‚æ•°: {len(unexpected_keys)} ä¸ª")
            
            # å†»ç»“å‚æ•°ï¼Œä½†ä¿æŒinit_cond_netå’Œcond_proj_inå¯è®­ç»ƒ
            for name, param in self.Roofdenoise_fn.named_parameters():
                if 'init_cond_net' in name or 'cond_proj_in' in name:
                    param.requires_grad = True
                    print(f"âœ… ä¿æŒå¯è®­ç»ƒ: {name}")
                else:
                    param.requires_grad = False
                
            print(f"âœ… Roofæ¨¡å‹åŠ è½½æˆåŠŸ: {roof_path}")
            return True
            
        except Exception as e:
            print(f"âŒ Roofæ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            print("è¯¦ç»†é”™è¯¯ä¿¡æ¯:")
            import traceback
            traceback.print_exc()
            return False

    def _load_roofline_model(self, roofline_path):
        """åŠ è½½rooflineæ¨¡å‹"""
        try:
            # è·å–rooflineç›®å½•è·¯å¾„
            roofline_dir = os.path.dirname(roofline_path)
            config_path = os.path.join(roofline_dir, 'config.json')
            
            if not os.path.exists(config_path):
                print(f"âŒ Rooflineé…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_path}")
                return False
                
            # è¯»å–é…ç½®æ–‡ä»¶
            with open(config_path, 'r') as f:
                roofline_config = json.load(f)
            
            # è·å–ç½‘ç»œé…ç½®
            network_config = roofline_config['model']['which_networks'][0]['args']
            unet_config = network_config['unet']
            module_name = network_config.get('module_name', 'roofline_modules')
            
            # ä¿å­˜å½“å‰çš„sys.pathçŠ¶æ€
            original_path = sys.path.copy()
            
            try:
                # å°†rooflineç›¸å…³è·¯å¾„æ·»åŠ åˆ°æœ€å‰é¢
                roofline_models_path = os.path.join(roofline_dir, 'models')
                sys.path.insert(0, roofline_models_path)
                sys.path.insert(0, roofline_dir)
                
                print(f"ğŸ”§ è®¾ç½®å¯¼å…¥è·¯å¾„: {roofline_models_path}")
                
                # ä½¿ç”¨ä¼ ç»Ÿå¯¼å…¥æ–¹å¼
                if module_name == 'roofline_modules':
                    from roofline_modules.unet import UNet as RooflineUNet
                elif module_name == 'guided_diffusion_modules':
                    from guided_diffusion_modules.unet import UNet as RooflineUNet
                else:
                    # é»˜è®¤å°è¯•roofline_modules
                    from roofline_modules.unet import UNet as RooflineUNet
                    
                print(f"âœ… æˆåŠŸå¯¼å…¥UNetæ¨¡å—: {module_name}")
                
            finally:
                # æ¢å¤åŸå§‹çš„sys.path
                sys.path = original_path
            
            # åˆ›å»ºrooflineæ¨¡å‹
            self.RoofLinedenoise_fn = RooflineUNet(**unet_config)
            print(f"âœ… æˆåŠŸåˆ›å»ºRooflineæ¨¡å‹")
            
            # åŠ è½½é¢„è®­ç»ƒæƒé‡
            checkpoint = torch.load(roofline_path, map_location='cpu')
            if 'model' in checkpoint:
                state_dict = checkpoint['model']
            elif 'state_dict' in checkpoint:
                state_dict = checkpoint['state_dict']
            else:
                state_dict = checkpoint
                
            # è¿‡æ»¤æƒé‡
            filtered_dict = {}
            for k, v in state_dict.items():
                if k.startswith('denoise_fn.'):
                    new_key = k.replace('denoise_fn.', '')
                    filtered_dict[new_key] = v
                elif 'denoise_fn' not in k:
                    filtered_dict[k] = v
                    
            # åŠ è½½æƒé‡ï¼Œä½¿ç”¨strict=Falseå¿½ç•¥ä¸åŒ¹é…çš„å‚æ•°
            missing_keys, unexpected_keys = self.RoofLinedenoise_fn.load_state_dict(filtered_dict, strict=False)
            
            # è®¡ç®—åŠ è½½æˆåŠŸç‡
            total_params = len(self.RoofLinedenoise_fn.state_dict())
            loaded_params = total_params - len(missing_keys)
            success_rate = loaded_params / total_params if total_params > 0 else 0
            
            print(f"âœ… Rooflineæƒé‡åŠ è½½å®Œæˆ: {loaded_params}/{total_params} å‚æ•° ({success_rate*100:.1f}%)")
            if missing_keys:
                print(f"âš ï¸  ç¼ºå¤±å‚æ•°: {len(missing_keys)} ä¸ª")
            if unexpected_keys:
                print(f"âš ï¸  å¤šä½™å‚æ•°: {len(unexpected_keys)} ä¸ª")
            

            for name, param in self.RoofLinedenoise_fn.named_parameters():
                if 'context_compressor' in name:
                    param.requires_grad = True
                    print(f"âœ… ä¿æŒå¯è®­ç»ƒ: {name}")
                else:
                    param.requires_grad = False
                
            print(f"âœ… Rooflineæ¨¡å‹åŠ è½½æˆåŠŸ: {roofline_path}")
            return True
            
        except Exception as e:
            print(f"âŒ Rooflineæ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            print("è¯¦ç»†é”™è¯¯ä¿¡æ¯:")
            import traceback
            traceback.print_exc()
            return False

    def set_loss(self, loss_fn):
        self.loss_fn = loss_fn
    

    def set_new_noise_schedule(self, device=torch.device('cuda'), phase='train',accelerator=None): #è®¾ç½®æ–°çš„å™ªå£°è°ƒåº¦è¡¨
        if accelerator is not None:
            self.accelerator = accelerator
        else:
            self.accelerator = None
        to_torch = partial(torch.tensor, dtype=torch.float32, device=device)#partialå‡½æ•°ï¼Œå›ºå®šå‡½æ•°çš„éƒ¨åˆ†å‚æ•°ï¼Œè¿”å›ä¸€ä¸ªæ–°çš„å‡½æ•°,è¿™é‡Œå›ºå®šäº†dtypeå’Œdevice
        betas = make_beta_schedule(**self.beta_schedule[phase])
        betas = betas.detach().cpu().numpy() if isinstance( #ä»è®¡ç®—å›¾ä¸­åˆ†ç¦»å‡ºæ¥ï¼Œä¸å†è·Ÿè¸ªå…¶æ¢¯åº¦
            betas, torch.Tensor) else betas
        alphas = 1. - betas

        timesteps, = betas.shape
        self.num_timesteps = int(timesteps)
        
        gammas = np.cumprod(alphas, axis=0)
        gammas_prev = np.append(1., gammas[:-1]) #å‰ä¸€ä¸ªgammaå€¼ï¼Œç¬¬ä¸€ä¸ªgammaå€¼ä¸º1ï¼Œç›´åˆ°gamma_t-1

        # calculations for diffusion q(x_t | x_{t-1}) and others
        self.register_buffer('gammas', to_torch(gammas))
        self.register_buffer('sqrt_recip_gammas', to_torch(np.sqrt(1. / gammas)))
        self.register_buffer('sqrt_recipm1_gammas', to_torch(np.sqrt(1. / gammas - 1)))
        #buggerä¸­çš„æ•°æ®ä¸ä¼šè¢«æ›´æ–°ï¼Œä¸ä¼šè¢«ä¼˜åŒ–å™¨æ›´æ–°ï¼Œä½†æ˜¯ä¼šè¢«ä¿å­˜å’ŒåŠ è½½ã€‚ç”¨å¤„ï¼šä¿å­˜æ¨¡å‹çš„å‚æ•°ï¼Œä½†æ˜¯ä¸ä¼šè¢«æ›´æ–°

        # calculations for posterior q(x_{t-1} | x_t, x_0) åéªŒåˆ†å¸ƒ
        posterior_variance = betas * (1. - gammas_prev) / (1. - gammas) #åéªŒæ–¹å·®ï¼Œsigma^2
        # below: log calculation clipped because the posterior variance is 0 at the beginning of the diffusion chain
        self.register_buffer('posterior_log_variance_clipped', to_torch(np.log(np.maximum(posterior_variance, 1e-20)))) #åéªŒæ–¹å·®çš„å¯¹æ•°ï¼Œclipæ˜¯ä¸ºäº†é˜²æ­¢å‡ºç°è´Ÿæ•°
        self.register_buffer('posterior_mean_coef1', to_torch(betas * np.sqrt(gammas_prev) / (1. - gammas))) 
        self.register_buffer('posterior_mean_coef2', to_torch((1. - gammas_prev) * np.sqrt(alphas) / (1. - gammas)))

    def _print_trainable_parameters(self):
        """æ‰“å°å„ç»„ä»¶çš„å¯è®­ç»ƒå‚æ•°ç»Ÿè®¡"""
        def count_parameters(model):
            return sum(p.numel() for p in model.parameters() if p.requires_grad)
        
        def count_total_parameters(model):
            return sum(p.numel() for p in model.parameters())
        
        roof_trainable = count_parameters(self.Roofdenoise_fn)
        roof_total = count_total_parameters(self.Roofdenoise_fn)
        
        roofline_trainable = count_parameters(self.RoofLinedenoise_fn)
        roofline_total = count_total_parameters(self.RoofLinedenoise_fn)
        
        mom_trainable = count_parameters(self.MOM)
        mom_total = count_total_parameters(self.MOM)
        
        total_trainable = roof_trainable + roofline_trainable + mom_trainable
        total_params = roof_total + roofline_total + mom_total
        
        print("\n" + "="*60)
        print("MODEL PARAMETER STATISTICS")
        print("="*60)
        print(f"Roof Denoiser:     {roof_trainable:,} / {roof_total:,} trainable")
        print(f"Roofline Denoiser: {roofline_trainable:,} / {roofline_total:,} trainable")
        print(f"MOM:               {mom_trainable:,} / {mom_total:,} trainable")
        print("-"*60)
        print(f"Total:             {total_trainable:,} / {total_params:,} trainable")
        print(f"Trainable ratio:   {total_trainable/total_params*100:.2f}%")
        print("="*60)
    
    def unfreeze_component(self, component_name):
        """
        è§£å†»æŒ‡å®šç»„ä»¶
        
        Args:
            component_name (str): 'roof', 'roofline', æˆ– 'mom'
        """
        if component_name.lower() == 'roof':
            for param in self.Roofdenoise_fn.parameters():
                param.requires_grad = True
            print("Roof denoiser unfrozen!")
        elif component_name.lower() == 'roofline':
            for param in self.RoofLinedenoise_fn.parameters():
                param.requires_grad = True
            print("Roofline denoiser unfrozen!")
        elif component_name.lower() == 'mom':
            for param in self.MOM.parameters():
                param.requires_grad = True
            print("MOM unfrozen!")
        else:
            print(f"Unknown component: {component_name}")
            
        self._print_trainable_parameters()
    
    def freeze_component(self, component_name):
        """
        å†»ç»“æŒ‡å®šç»„ä»¶
        
        Args:
            component_name (str): 'roof', 'roofline', æˆ– 'mom'
        """
        if component_name.lower() == 'roof':
            for name, param in self.Roofdenoise_fn.named_parameters():
                if 'init_cond_net' in name or 'cond_proj_in' in name:
                    param.requires_grad = True
                    print(f"âœ… ä¿æŒå¯è®­ç»ƒ: {name}")
                else:
                    param.requires_grad = False
            print("Roof denoiser frozen (except init_cond_net and cond_proj_in)!")
        elif component_name.lower() == 'roofline':
            for param in self.RoofLinedenoise_fn.parameters():
                param.requires_grad = False
            print("Roofline denoiser frozen!")
        elif component_name.lower() == 'mom':
            for param in self.MOM.parameters():
                param.requires_grad = False
            print("MOM frozen!")
        else:
            print(f"Unknown component: {component_name}")
            
        self._print_trainable_parameters()

    def freeze_component_completely(self, component_name):
        """
        å®Œå…¨å†»ç»“æŒ‡å®šç»„ä»¶ï¼ˆåŒ…æ‹¬init_cond_netå’Œcond_proj_inï¼‰
        
        Args:
            component_name (str): 'roof', 'roofline', æˆ– 'mom'
        """
        if component_name.lower() == 'roof':
            for param in self.Roofdenoise_fn.parameters():
                param.requires_grad = False
            print("Roof denoiser completely frozen!")
        elif component_name.lower() == 'roofline':
            for param in self.RoofLinedenoise_fn.parameters():
                param.requires_grad = False
            print("Roofline denoiser completely frozen!")
        elif component_name.lower() == 'mom':
            for param in self.MOM.parameters():
                param.requires_grad = False
            print("MOM completely frozen!")
        else:
            print(f"Unknown component: {component_name}")
            
        self._print_trainable_parameters()

    def predict_start_from_noise(self, y_t, t, noise): #é¢„æµ‹åˆå§‹å€¼y0_hatï¼Œç”¨ytå’Œå™ªå£°é¢„æµ‹y0_hat
        return (
            extract(self.sqrt_recip_gammas, t, y_t.shape) * y_t -      #extractå°†aä¸­çš„å€¼å–å‡ºæ¥ï¼Œé‡‡æ ·tï¼Œreshapeæˆb,1,1,1
            extract(self.sqrt_recipm1_gammas, t, y_t.shape) * noise
        )

    def predict_start_from_noise_with_gamma(self,y_t, gamma, noise):
        """
        ä½¿ç”¨æä¾›çš„ gamma å€¼ï¼ˆ\bar{\alpha}_tï¼‰é¢„æµ‹åˆå§‹æ•°æ® \hat{y}_0ã€‚
        å‚æ•°ï¼š
            y_t: åŠ å™ªæ•°æ®ï¼Œå½¢çŠ¶ä¸º (batch_size, channels, height, width)
            gamma: \bar{\alpha}_t å€¼ï¼Œå½¢çŠ¶ä¸º (batch_size,) æˆ–æ ‡é‡ï¼Œè¡¨ç¤ºç´¯ç§¯ alpha
            noise: é¢„æµ‹çš„å™ªå£°ï¼Œå½¢çŠ¶ä¸ y_t ç›¸åŒ
        è¿”å›ï¼š
            y0_hat: é¢„æµ‹çš„åˆå§‹æ•°æ®ï¼Œå½¢çŠ¶ä¸ y_t ç›¸åŒ
        """
        # ç¡®ä¿ gamma æ˜¯å¼ é‡å¹¶ç§»åŠ¨åˆ° y_t çš„è®¾å¤‡
        if not isinstance(gamma, torch.Tensor):
            gamma = torch.tensor(gamma, dtype=torch.float32, device=y_t.device)
        
        # å¦‚æœ gamma æ˜¯æ ‡é‡ï¼Œæ‰©å±•ä¸º (batch_size,)
        if gamma.dim() == 0:
            gamma = gamma.expand(y_t.shape[0])
        
        # ç¡®ä¿ gamma å½¢çŠ¶ä¸º (batch_size,)
        gamma = gamma.view(-1)
        
        # è®¡ç®— sqrt_recip_gammas = 1 / sqrt(gamma)
        sqrt_recip_gammas = 1.0 / torch.sqrt(gamma)
        
        # è®¡ç®— sqrt_recipm1_gammas = sqrt((1 - gamma) / gamma)
        sqrt_recipm1_gammas = torch.sqrt((1.0 - gamma) / gamma)
        
        # è°ƒæ•´å½¢çŠ¶ä»¥åŒ¹é… y_t çš„å½¢çŠ¶ï¼Œä¾‹å¦‚ (batch_size, 1, 1, 1)
        sqrt_recip_gammas = sqrt_recip_gammas.view(-1, 1, 1, 1)
        sqrt_recipm1_gammas = sqrt_recipm1_gammas.view(-1, 1, 1, 1)
        
        # è®¡ç®— \hat{y}_0 = (y_t - sqrt(1 - \bar{\alpha}_t) * noise) / sqrt(\bar{\alpha}_t)
        y0_hat = sqrt_recip_gammas * y_t - sqrt_recipm1_gammas * noise
        
        return y0_hat
    

    def q_posterior(self, y_0_hat, y_t, t): #pï¼ˆxt-1|xt,x0_hatï¼‰#åéªŒåˆ†å¸ƒxt-1|xt,y0_hat
        posterior_mean = (
            extract(self.posterior_mean_coef1, t, y_t.shape) * y_0_hat +
            extract(self.posterior_mean_coef2, t, y_t.shape) * y_t
        )
        posterior_log_variance_clipped = extract(self.posterior_log_variance_clipped, t, y_t.shape)
        return posterior_mean, posterior_log_variance_clipped

    def p_mean_variance(self, y_t, t, clip_denoised: bool, y_cond=None):  
        noise_level = extract(self.gammas, t, x_shape=(1, 1)).to(y_t.device) #å™ªå£°æ°´å¹³ç´¯ç§¯ï¼Œä»gammaä¸­æå–tæ—¶åˆ»çš„å™ªå£°æ°´å¹³
        y_0_hat = self.predict_start_from_noise(                             #é¢„æµ‹åˆå§‹å€¼y0_hat
                y_t, t=t, noise=self.denoise_fn(torch.cat([y_cond, y_t], dim=1), noise_level)) #æ”¾å…¥unetï¼Œå¾—åˆ°å¸¦å™ªå£°è¾“å‡º
        

        if clip_denoised:   #å°†y0_hatçš„å€¼é™åˆ¶åœ¨-1åˆ°1ä¹‹é—´
            y_0_hat.clamp_(-1., 1.)

        model_mean, posterior_log_variance = self.q_posterior(
            y_0_hat=y_0_hat, y_t=y_t, t=t) #é¢„æµ‹åéªŒåˆ†å¸ƒyt-1|yt,y0_hat
        return model_mean, posterior_log_variance

    def q_sample(self, y_0, sample_gammas, noise=None):    #æ­£å‘é‡‡æ ·ï¼Œä»y0é‡‡æ ·åˆ°yt
        noise = default(noise, lambda: torch.randn_like(y_0))
        return (
            sample_gammas.sqrt() * y_0 +
            (1 - sample_gammas).sqrt() * noise
        )
 
    @torch.no_grad()  #é‡‡æ ·è¿‡ç¨‹ä¸éœ€è¦æ¢¯åº¦
    def p_sample(self, y_t, t, clip_denoised=True, y_cond=None):  #é€†å‘é‡‡æ ·ï¼Œä»yté‡‡æ ·åˆ°yt-1
        model_mean, model_log_variance = self.p_mean_variance(
            y_t=y_t, t=t, clip_denoised=clip_denoised, y_cond=y_cond)
        noise = torch.randn_like(y_t) if any(t>0) else torch.zeros_like(y_t) #éšæœºå™ªå£°
        return model_mean + noise * (0.5 * model_log_variance).exp()   #mean + noise * std. å› ä¸ºmodel_log_varianceå®é™…ä¸Šæ˜¯log(std^2)ï¼Œè¿™é‡Œä¹˜ä»¥noiseçš„æ˜¯std
    
    @torch.no_grad()
    def p_sample_ddim_rl(self, y_t, t,t_prev, clip_denoised=True, y_cond=None, eta=0.2,control_image=None, self_cond=None): #DDIMé‡‡æ ·ï¼Œä»yté‡‡æ ·åˆ°yt-1
        # 1. é¢„æµ‹ yâ‚€_hatï¼ˆåˆå§‹å›¾åƒä¼°è®¡ï¼‰
        noise_level = extract(self.gammas, t, x_shape=(1, 1)).to(y_t.device)
        # ä½¿ç”¨RoofLinedenoise_fnè¿›è¡Œrooflineé‡‡æ ·
        model_output = self.RoofLinedenoise_fn(torch.cat([y_cond, y_t], dim=1), noise_level, control_image, self_cond=self_cond)

        if self.parameterization == "v":
            y0_hat = self.predict_start_from_v(y_t, t, model_output)
        else:
            y0_hat = self.predict_start_from_noise(y_t, t, model_output)

        if clip_denoised:
            y0_hat.clamp_(-1., 1.)

        # 2. æ ¹æ®å½“å‰æ—¶é—´æ­¥è®¡ç®—Î±_tå’Œç›¸å…³é‡
        alpha_t = extract(self.gammas, t, y_t.shape)
        sqrt_alpha_t = torch.sqrt(alpha_t)
        sqrt_one_minus_alpha_t = torch.sqrt(1 - alpha_t)
        # æ ¹æ®å…¬å¼ï¼Œé¢„æµ‹å™ªå£°Îµ_pred
        epsilon_pred = (y_t - sqrt_alpha_t * y0_hat) / sqrt_one_minus_alpha_t

        # 3. è·å–ä¸Šä¸€æ—¶é—´æ­¥çš„Î±ï¼ˆæ³¨æ„å¤„ç†t=0çš„è¾¹ç•Œæƒ…å†µï¼‰
        # å‡è®¾tä¸ºå½¢çŠ¶ä¸º(b,)çš„å¼ é‡ï¼Œå–ç¬¬ä¸€ä¸ªå…ƒç´ åˆ¤æ–­æ˜¯å¦ä¸º0
        if t_prev[0].item()>=0:
            alpha_prev = extract(self.gammas, t_prev, y_t.shape)
        else:
            # t==0æ—¶ç›´æ¥è¿”å›y0_hatï¼ˆæœ€åä¸€æ­¥ï¼‰
            return y0_hat, y0_hat

        # 4. è®¡ç®—DDIMä¸­çš„Ïƒ_t, æ§åˆ¶éšæœºæ€§
        sigma_t = eta * torch.sqrt((1 - alpha_prev) / (1 - alpha_t)) * torch.sqrt(1 - alpha_t / alpha_prev)

        # 5. DDIMæ›´æ–°å…¬å¼ï¼šç”±ç¡®å®šæ€§éƒ¨åˆ†å’Œéšæœºéƒ¨åˆ†æ„æˆ
        y_t_prev = (torch.sqrt(alpha_prev) * y0_hat +
                    torch.sqrt(1 - alpha_prev - sigma_t**2) * epsilon_pred)
        if eta > 0:
            y_t_prev = y_t_prev + sigma_t * torch.randn_like(y_t)

        return y_t_prev, y0_hat
    
    @torch.no_grad()
    def p_sample_ddim_r(self, y_t, t,t_prev, clip_denoised=True, y_cond=None, eta=0.0,control_image=None):  # ä½¿ç”¨ddimè¿›è¡Œé€†å‘é‡‡æ ·
        # 1. é¢„æµ‹ yâ‚€_hatï¼ˆåˆå§‹å›¾åƒä¼°è®¡ï¼‰
        # noise_level = extract(self.gammas, t, x_shape=(1, 1)).to(y_t.device)
        # y0_hat = self.predict_start_from_noise(
        #     y_t, t=t, noise=self.denoise_fn(torch.cat([y_cond, y_t], dim=1), noise_level,control_image)
        # )

        noise_level = extract(self.gammas, t, x_shape=(1, 1)).to(y_t.device)
        # ä½¿ç”¨Roofdenoise_fnè¿›è¡Œroofé‡‡æ ·
        model_output = self.Roofdenoise_fn(torch.cat([y_cond, y_t], dim=1), noise_level, control_image)

        if self.parameterization == "v":
            # Model predicts v, compute xâ‚€_hat from v
            y0_hat = self.predict_start_from_v(y_t, t, model_output)
        else:
            # Model predicts noise Îµ, compute xâ‚€_hat from noise
            y0_hat = self.predict_start_from_noise(y_t, t, model_output)

        if clip_denoised:
            y0_hat.clamp_(-1., 1.)


        # 2. æ ¹æ®å½“å‰æ—¶é—´æ­¥è®¡ç®—Î±_tå’Œç›¸å…³é‡
        alpha_t = extract(self.gammas, t, y_t.shape)
        sqrt_alpha_t = torch.sqrt(alpha_t)
        sqrt_one_minus_alpha_t = torch.sqrt(1 - alpha_t)
        # æ ¹æ®å…¬å¼ï¼Œé¢„æµ‹å™ªå£°Îµ_pred
        epsilon_pred = (y_t - sqrt_alpha_t * y0_hat) / sqrt_one_minus_alpha_t

        # 3. è·å–ä¸Šä¸€æ—¶é—´æ­¥çš„Î±ï¼ˆæ³¨æ„å¤„ç†t=0çš„è¾¹ç•Œæƒ…å†µï¼‰
        # å‡è®¾tä¸ºå½¢çŠ¶ä¸º(b,)çš„å¼ é‡ï¼Œå–ç¬¬ä¸€ä¸ªå…ƒç´ åˆ¤æ–­æ˜¯å¦ä¸º0
        if t_prev[0].item()>=0:
            alpha_prev = extract(self.gammas, t_prev, y_t.shape)
        else:
            # t==0æ—¶ç›´æ¥è¿”å›y0_hatï¼ˆæœ€åä¸€æ­¥ï¼‰
            return y0_hat, y0_hat

        # 4. è®¡ç®—DDIMä¸­çš„Ïƒ_t, æ§åˆ¶éšæœºæ€§
        sigma_t = eta * torch.sqrt((1 - alpha_prev) / (1 - alpha_t)) * torch.sqrt(1 - alpha_t / alpha_prev)

        # 5. DDIMæ›´æ–°å…¬å¼ï¼šç”±ç¡®å®šæ€§éƒ¨åˆ†å’Œéšæœºéƒ¨åˆ†æ„æˆ
        y_t_prev = (torch.sqrt(alpha_prev) * y0_hat +
                    torch.sqrt(1 - alpha_prev - sigma_t**2) * epsilon_pred)
        if eta > 0:
            y_t_prev = y_t_prev + sigma_t * torch.randn_like(y_t)

        return y_t_prev, y0_hat

    def predict_start_from_v(self, y_t, t, v):
        """
        ä»vå‚æ•°åŒ–é¢„æµ‹åˆå§‹æ•°æ®
        """
        sqrt_alpha_t = extract(torch.sqrt(self.gammas), t, y_t.shape)
        sqrt_one_minus_alpha_t = extract(torch.sqrt(1. - self.gammas), t, y_t.shape)
        return sqrt_alpha_t * y_t - sqrt_one_minus_alpha_t * v

    @torch.no_grad()
    def restoration(self, y_cond, y_t=None, y_0=None, mask=None, sample_num=8, ddim_sample_steps=50, roofline_image=None):
        """
        ä½¿ç”¨ DDIM é‡‡æ ·è¿›è¡Œå›¾åƒæ¢å¤ã€‚
        å‚æ•°ï¼š
            y_cond: å·²çŸ¥æ¡ä»¶éƒ¨åˆ†
            y_t: åˆå§‹å™ªå£°ï¼ˆå¦‚æœæ²¡æœ‰æä¾›ï¼Œåˆ™éšæœºåˆå§‹åŒ–ï¼‰
            y_0: åŸå§‹å›¾åƒï¼ˆç”¨äºè¡¥å…¨ä»»åŠ¡ï¼‰
            mask: å·²çŸ¥åŒºåŸŸçš„æ©ç ï¼ˆç”¨äºè¡¥å…¨ä»»åŠ¡ï¼‰
            roofline_image: rooflineå›¾åƒ
        """
        # æ£€æŸ¥é¢„è®­ç»ƒæ¨¡å‹æ˜¯å¦å·²åŠ è½½
        if self.Roofdenoise_fn is None or self.RoofLinedenoise_fn is None:
            raise RuntimeError("Pretrained models (roof and roofline) must be loaded before inference. "
                             "Call load_pretrained_models() first.")
        
        device = self.accelerator.device if self.accelerator is not None else y_cond.device
        b, *_ = y_cond.shape
        
        # æ„å»º DDIM é‡‡æ ·çš„æ—¶é—´æ­¥åºåˆ—
        ddim_steps = ddim_sample_steps
        timesteps = torch.linspace(0, self.num_timesteps - 1, steps=ddim_steps, dtype=torch.long, device=y_cond.device)
        timesteps = timesteps.flip(0)  # é€†åºé‡‡æ ·ï¼Œä»é«˜åˆ°ä½
        
        # timesteps_prev
        timesteps_prev = torch.cat([timesteps, torch.tensor([-1], device=device, dtype=torch.long)])
        timesteps_prev = timesteps_prev[1:]  # å»æ‰ç¬¬ä¸€ä¸ª
        
        # åˆå§‹åŒ–å™ªå£°
        y_t_roof = default(y_t, lambda: torch.randn_like(y_cond))
        y_t_roof = y_t_roof*mask+(1.-mask)*y_0
        y_t_roofline = default(y_t, lambda: torch.randn_like(roofline_image))
        y_0_hat_roofline = torch.zeros_like(roofline_image)  # åˆå§‹åŒ–y0_hat_rooflineä¸º0
        
        # ä¿å­˜é‡‡æ ·çš„å›¾åƒ
        ret_arr_roof = [y_t_roof]
        ret_arr_roofline = [y_t_roofline]
        ret_arr_roofline_y0 = [y_0_hat_roofline]
        
        for t_val, t_prev in zip(timesteps, timesteps_prev):
            t_tensor = torch.full((b,), t_val.item(), device=device, dtype=torch.long)
            t_tensor_prev = torch.full((b,), t_prev.item(), device=device, dtype=torch.long)
            
            # å‡†å¤‡MOMè¾“å…¥ï¼šä¸Šä¸€æ­¥çš„y_t_roofå’Œy_0_hat_rooflineï¼ˆå½’ä¸€åŒ–åˆ°[0,1]ï¼‰
            y_0_hat_roofline_norm = (y_0_hat_roofline + 1.) / 2.
            
            # MOM é¢„æµ‹
            mom_roof, mom_roofline = self.MOM(
                torch.cat([y_cond, y_t_roof, y_0_hat_roofline_norm*mask], dim=1), 
                extract(self.gammas, t_tensor, x_shape=(1, 1)).to(device)
            )
            
            # Roofline é‡‡æ ·
            y_t_roofline, y_0_hat_roofline = self.p_sample_ddim_rl(
                y_t_roofline, t_tensor, t_tensor_prev, 
                y_cond=y_cond, eta=0.0, control_image=y_t_roof,self_cond=y_0_hat_roofline_norm
            )
            
            # Roof é‡‡æ ·
            y_t_roof, _ = self.p_sample_ddim_r(
                y_t_roof, t_tensor, t_tensor_prev, 
                y_cond=y_cond, control_image=y_0_hat_roofline
            )
            
            # å¦‚æœæ˜¯è¡¥å…¨ä»»åŠ¡ï¼Œä¿ç•™å·²çŸ¥åŒºåŸŸ
            if mask is not None and y_0 is not None:
                y_t_roof = y_0 * (1. - mask) + mask * y_t_roof
                y_t_roofline = roofline_image * (1. - mask) + mask * y_t_roofline
            
            # ä¿å­˜é‡‡æ ·çš„å›¾åƒ
            ret_arr_roof.append(y_t_roof)
            ret_arr_roofline.append(y_t_roofline)
            ret_arr_roofline_y0.append(y_0_hat_roofline)
        
        # æ‹¼æ¥æ‰€æœ‰é‡‡æ ·çš„ä¸­é—´ç»“æœ
        ret_arr_roof = torch.cat(ret_arr_roof, dim=0)
        ret_arr_roofline = torch.cat(ret_arr_roofline, dim=0)
        ret_arr_roofline_y0 = torch.cat(ret_arr_roofline_y0, dim=0)
        
        return y_t_roof, ret_arr_roof, ret_arr_roofline, ret_arr_roofline_y0, y_0_hat_roofline



    def forward(self, y_0, y_cond=None, mask=None, noise=None,roofline_image=None): #è®­ç»ƒè¿‡ç¨‹
        '''
        y_0: heightmap
        y_cond: corrupted heightmap
        mask: mask for corrupted heightmap
        noise: noise for y_0, default is None, if None, use random noise
        roofline_image: roofline image for roof completion, default is None, if None, use y_cond'''
        # sampling from p(gammas)
        b, *_ = y_0.shape #bæ˜¯batch_size
        t = torch.randint(1, self.num_timesteps, (b,), device=y_0.device).long() #éšæœºé‡‡æ ·ä¸€ä¸ªæ—¶é—´æ­¥ï¼Œå½¢çŠ¶ä¸ºbï¼Œ
        gamma_t1 = extract(self.gammas, t-1, x_shape=(1, 1)) #æå–t-1æ—¶åˆ»çš„gammaï¼Œå½¢çŠ¶ä¸ºb,-ã€‹b,1,1
        sqrt_gamma_t2 = extract(self.gammas, t, x_shape=(1, 1))    #æå–tæ—¶åˆ»çš„gammaï¼Œå½¢çŠ¶ä¸ºb,-ã€‹b,1,1
        sample_gammas = (sqrt_gamma_t2-gamma_t1) * torch.rand((b, 1), device=y_0.device) + gamma_t1  
        #è¿™ä¸€æ­¥çš„ç›®çš„æ˜¯åœ¨ä¸¤ä¸ªè¿ç»­æ—¶é—´æ­¥ä¹‹é—´éšæœºé‡‡æ ·ä¸€ä¸ªå€¼ï¼Œä½¿å¾—è®­ç»ƒä¸­ä½¿ç”¨çš„ gamma å‚æ•°æœ‰ä¸€å®šçš„è¿ç»­æ€§å’Œéšæœºæ€§ã€‚
        sample_gammas = sample_gammas.view(b, -1)#å°†é‡‡æ ·çš„gammaå€¼reshapeæˆb,1

        noise_roof = default(noise, lambda: torch.randn_like(y_0)) #heightmapçš„å™ªå£°
        y_noisy = self.q_sample(
            y_0=y_0, sample_gammas=sample_gammas.view(-1, 1, 1, 1), noise=noise_roof) #åŠ å™ªåçš„å›¾åƒ
        
        noise_roofline = default(noise, lambda: torch.randn_like(roofline_image)) #rooflineçš„å™ªå£°
        y_roofline_noisy = self.q_sample(
            y_0=roofline_image, sample_gammas=sample_gammas.view(-1, 1, 1, 1), noise=noise_roofline) #åŠ å™ªåçš„rooflineå›¾åƒ

        if mask is not None:
            #noise_hat = self.denoise_fn(torch.cat([y_cond, y_noisy*mask+(1.-mask)*y_0], dim=1), sample_gammas,control_image)
            with torch.no_grad(): # This no_grad() is for the first roofline prediction which is not dependent on MOM
                roofline_output1 = self.RoofLinedenoise_fn(torch.cat([y_cond, y_roofline_noisy*mask+(1.-mask)*y_0], dim=1), sample_gammas,y_noisy,self_cond=torch.zeros_like(y_noisy)) #ä»y_condå’Œy_roofline_noisyä¸­å»å™ªå£°ï¼Œé€šè¿‡unet
                if self.parameterization == "v":
                    y_roofline_0_hat_selfcond = self.predict_start_from_v(y_roofline_noisy, t, roofline_output1)
                else:
                    # å¦‚æœä¸æ˜¯vå‚æ•°åŒ–ï¼Œä½¿ç”¨é»˜è®¤çš„å™ªå£°é¢„æµ‹æ–¹æ³•
                    y_roofline_0_hat_selfcond = self.predict_start_from_noise_with_gamma(y_roofline_noisy, sample_gammas, roofline_output1)
            

            y_roofline_0_hat_selfcond_norm = (y_roofline_0_hat_selfcond + 1.) / 2.
            
            mom_roof,mom_roofline = self.MOM(torch.cat([y_cond, y_noisy*mask+(1.-mask)*y_0,y_roofline_0_hat_selfcond_norm*mask+(1.-mask)], dim=1), sample_gammas,) #ä»y_condå’Œy_noisyä¸­å»å™ªå£°ï¼Œé€šè¿‡unet

            # Removed torch.no_grad() here
            roofline_output2 = self.RoofLinedenoise_fn(torch.cat([y_cond, y_roofline_noisy*mask+(1.-mask)*y_0], dim=1), sample_gammas, mom_roof,self_cond=roofline_output1) #ä»y_condå’Œy_roofline_noisyä¸­å»å™ªå£°ï¼Œé€šè¿‡unet
            roof_output = self.Roofdenoise_fn(torch.cat([y_cond, y_noisy*mask+(1.-mask)*y_0], dim=1), sample_gammas,mom_roofline) #ä»y_condå’Œy_noisyä¸­å»å™ªå£°ï¼Œé€šè¿‡unet
            
            # The prediction of y_roofline_y0_hat should also not be in no_grad() if roofline_output2 requires grad
            if self.parameterization == "v":
                y_roofline_y0_hat = self.predict_start_from_v(y_roofline_noisy, t, roofline_output2)
            else:
                y_roofline_y0_hat = self.predict_start_from_noise_with_gamma(y_roofline_noisy, sample_gammas, roofline_output2)
            

            if hasattr(self.loss_fn, '__name__') and self.loss_fn.__name__ in ['masked_mse_loss', 'masked_l1_loss', 'combined_loss_with_masked_ssim']:
                #loss = self.loss_fn(mask*noise, mask*noise_hat, mask) # might not be necessary 
                loss_roof_noise = self.loss_fn(mask*noise_roof, mask*roof_output, mask) # might not be necessary
                loss_roofline_noise = self.loss_fn(mask*noise_roofline, mask*roofline_output2, mask) # might not be necessary
                loss_roofline_recon = self.loss_fn(mask*roofline_image, mask* y_roofline_y0_hat, mask) # might not be necessary
                loss = loss_roof_noise + loss_roofline_noise + loss_roofline_recon
        else:
            noise_hat = self.denoise_fn(torch.cat([y_cond, y_noisy], dim=1), sample_gammas) #ä»y_condå’Œy_noisyä¸­å»å™ªå£°ï¼Œé€šè¿‡unet
            #maskä¸å­˜åœ¨æ—¶ï¼Œç›´æ¥å»å™ªå£°ï¼Œä¸éœ€è¦ä¿ç•™maskéƒ¨åˆ†ã€‚ä¹Ÿå°±æ˜¯è¯´æ­¤æ—¶çš„y_noisyåœ¨maskå¤–çš„éƒ¨åˆ†ä¹Ÿæœ‰å™ªå£°
            loss = self.loss_fn(noise, noise_hat)
            # å½“maskä¸ºNoneæ—¶ï¼Œè®¾ç½®é»˜è®¤å€¼
            loss_roof_noise = loss
            loss_roofline_noise = loss
            loss_roofline_recon = loss
            y_roofline_y0_hat = None
        return loss,{"loss_roof_noise":loss_roof_noise,"loss_roofline_noise":loss_roofline_noise,"loss_roofline_recon":loss_roofline_recon} ,y_roofline_y0_hat#è¿”å›æŸå¤±å’Œå…¶ä»–ä¿¡æ¯ï¼Œæ–¹ä¾¿åç»­çš„è¯„ä¼°å’Œä¿å­˜æ¨¡å‹


# gaussian diffusion trainer class
def exists(x):
    return x is not None

def default(val, d):
    if exists(val): #å¦‚æœvalå­˜åœ¨ï¼Œè¿”å›valï¼Œå¦åˆ™è¿”å›d
        return val
    return d() if isfunction(d) else d

def extract(a, t, x_shape=(1,1,1,1)): #aæ˜¯åˆ—è¡¨ï¼Œtæ˜¯æ—¶é—´æ­¥ï¼Œx_shapeæ˜¯å¼ é‡çš„å½¢çŠ¶
    # 
    b, *_ = t.shape #æ˜Ÿå·è¡¨ç¤ºè§£åŒ…æˆä¸€ä¸ªåˆ—è¡¨ï¼Œbæ˜¯tçš„ç¬¬ä¸€ä¸ªç»´åº¦ï¼Œbatch_sizeå½¢çŠ¶ï¼ˆbï¼Œï¼‰
    out = a.gather(-1, t) #åœ¨açš„æœ€åä¸€ä¸ªç»´åº¦ä¸Šï¼Œæ ¹æ®tçš„ç´¢å¼•å€¼ï¼Œå°†aä¸­çš„å€¼å–å‡ºæ¥ã€‚ä¸tå½¢çŠ¶ç›¸åŒ
    return out.reshape(b, *((1,) * (len(x_shape) - 1))) #(1,) * (len(x_shape) - 1)-ã€‹(1,1,1)ï¼Œæ˜Ÿå·è¡¨ç¤ºè§£åŒ…æˆä¸€ä¸ªåˆ—è¡¨å¹¶ä¼ å…¥å‡½æ•°ï¼Œç­‰ä»·äºreshape(b,1,1,1)

# beta_schedule function
def _warmup_beta(linear_start, linear_end, n_timestep, warmup_frac):
    betas = linear_end * np.ones(n_timestep, dtype=np.float64)
    warmup_time = int(n_timestep * warmup_frac)
    betas[:warmup_time] = np.linspace(
        linear_start, linear_end, warmup_time, dtype=np.float64)
    return betas

def make_beta_schedule(schedule, n_timestep, linear_start=1e-6, linear_end=1e-2, cosine_s=8e-3):
    if schedule == 'quad':
        betas = np.linspace(linear_start ** 0.5, linear_end ** 0.5,
                            n_timestep, dtype=np.float64) ** 2
    elif schedule == 'linear':
        betas = np.linspace(linear_start, linear_end,
                            n_timestep, dtype=np.float64)
    elif schedule == 'warmup10':
        betas = _warmup_beta(linear_start, linear_end,
                             n_timestep, 0.1)
    elif schedule == 'warmup50':
        betas = _warmup_beta(linear_start, linear_end,
                             n_timestep, 0.5)
    elif schedule == 'const':
        betas = linear_end * np.ones(n_timestep, dtype=np.float64)
    elif schedule == 'jsd':  # 1/T, 1/(T-1), 1/(T-2), ..., 1
        betas = 1. / np.linspace(n_timestep,
                                 1, n_timestep, dtype=np.float64)
    elif schedule == "cosine":
        timesteps = (
            torch.arange(n_timestep + 1, dtype=torch.float64) /
            n_timestep + cosine_s
        )
        alphas = timesteps / (1 + cosine_s) * math.pi / 2
        alphas = torch.cos(alphas).pow(2)
        alphas = alphas / alphas[0]
        betas = 1 - alphas[1:] / alphas[:-1]
        betas = betas.clamp(max=0.999)
    else:
        raise NotImplementedError(schedule)
    return betas


