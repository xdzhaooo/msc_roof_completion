#!/usr/bin/env python3
"""
AutoEncoderå¿«é€Ÿå¼€å§‹è„šæœ¬
ç”¨äºæµ‹è¯•æ¨¡å‹æ¶æ„å’Œæ•°æ®åŠ è½½æ˜¯å¦æ­£å¸¸å·¥ä½œ
"""

import torch
import numpy as np
from pathlib import Path
import argparse

from model import AutoEncoder, count_parameters
from dataset import DepthImageDataset, create_dataloader


def test_model():
    """æµ‹è¯•æ¨¡å‹æ¶æ„"""
    print("=" * 50)
    print("æµ‹è¯•æ¨¡å‹æ¶æ„...")
    
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")
    
    # åˆ›å»ºæ¨¡å‹
    model = AutoEncoder(input_channels=1, latent_dim=768, output_size=128)
    model = model.to(device)
    
    print(f"æ¨¡å‹å‚æ•°æ•°é‡: {count_parameters(model):,}")
    
    # æµ‹è¯•å‰å‘ä¼ æ’­
    batch_size = 2
    test_input = torch.randn(batch_size, 1, 128, 128).to(device)
    
    print(f"è¾“å…¥å½¢çŠ¶: {test_input.shape}")
    
    # å®Œæ•´å‰å‘ä¼ æ’­
    with torch.no_grad():
        reconstructed, latent = model(test_input)
        print(f"é‡å»ºå›¾åƒå½¢çŠ¶: {reconstructed.shape}")
        print(f"æ½œåœ¨å‘é‡å½¢çŠ¶: {latent.shape}")
        
        # æµ‹è¯•å•ç‹¬çš„ç¼–ç å’Œè§£ç 
        encoded = model.encode(test_input)
        decoded = model.decode(encoded)
        print(f"ç¼–ç å½¢çŠ¶: {encoded.shape}")
        print(f"è§£ç å½¢çŠ¶: {decoded.shape}")
    
    print("âœ… æ¨¡å‹æ¶æ„æµ‹è¯•é€šè¿‡!")
    return True


def test_dataset(data_dir: str = None):
    """æµ‹è¯•æ•°æ®é›†åŠ è½½"""
    print("=" * 50)
    print("æµ‹è¯•æ•°æ®é›†...")
    
    if data_dir is None or not Path(data_dir).exists():
        print("âŒ æœªæä¾›æœ‰æ•ˆçš„æ•°æ®ç›®å½•ï¼Œè·³è¿‡æ•°æ®é›†æµ‹è¯•")
        print("è¯·ä½¿ç”¨ --data_dir å‚æ•°æŒ‡å®šæ•°æ®ç›®å½•")
        return False
    
    try:
        # åˆ›å»ºæ•°æ®é›†
        dataset = DepthImageDataset(
            data_dir=data_dir,
            image_size=128,
            augment=True,
            height_range=10.0
        )
        
        print(f"æ•°æ®é›†å¤§å°: {len(dataset)}")
        
        if len(dataset) == 0:
            print("âŒ æ•°æ®é›†ä¸ºç©ºï¼Œè¯·æ£€æŸ¥æ•°æ®ç›®å½•")
            return False
        
        # æµ‹è¯•è·å–å•ä¸ªæ ·æœ¬
        sample = dataset[0]
        print(f"æ ·æœ¬å½¢çŠ¶: {sample.shape}")
        print(f"æ ·æœ¬æ•°æ®ç±»å‹: {sample.dtype}")
        print(f"æ ·æœ¬æ•°å€¼èŒƒå›´: [{sample.min():.3f}, {sample.max():.3f}]")
        
        # æµ‹è¯•æ•°æ®åŠ è½½å™¨
        dataloader = create_dataloader(
            data_dir=data_dir,
            batch_size=4,
            shuffle=True,
            num_workers=0,  # é¿å…å¤šè¿›ç¨‹é—®é¢˜
            augment=True
        )
        
        print(f"æ•°æ®åŠ è½½å™¨æ‰¹æ¬¡æ•°: {len(dataloader)}")
        
        # æµ‹è¯•è·å–ä¸€ä¸ªæ‰¹æ¬¡
        batch = next(iter(dataloader))
        print(f"æ‰¹æ¬¡å½¢çŠ¶: {batch.shape}")
        print(f"æ‰¹æ¬¡æ•°å€¼èŒƒå›´: [{batch.min():.3f}, {batch.max():.3f}]")
        
        print("âœ… æ•°æ®é›†æµ‹è¯•é€šè¿‡!")
        return True
        
    except Exception as e:
        print(f"âŒ æ•°æ®é›†æµ‹è¯•å¤±è´¥: {e}")
        return False


def test_training_step(data_dir: str = None):
    """æµ‹è¯•ä¸€ä¸ªè®­ç»ƒæ­¥éª¤"""
    print("=" * 50)
    print("æµ‹è¯•è®­ç»ƒæ­¥éª¤...")
    
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
    # åˆ›å»ºæ¨¡å‹
    model = AutoEncoder(input_channels=1, latent_dim=768, output_size=128)
    model = model.to(device)
    
    # åˆ›å»ºä¼˜åŒ–å™¨å’ŒæŸå¤±å‡½æ•°
    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)
    criterion = torch.nn.MSELoss()
    
    if data_dir and Path(data_dir).exists():
        # ä½¿ç”¨çœŸå®æ•°æ®
        try:
            dataloader = create_dataloader(
                data_dir=data_dir,
                batch_size=2,
                shuffle=True,
                num_workers=0,
                augment=False
            )
            data = next(iter(dataloader))
        except:
            # å¦‚æœçœŸå®æ•°æ®åŠ è½½å¤±è´¥ï¼Œä½¿ç”¨éšæœºæ•°æ®
            data = torch.randn(2, 1, 128, 128)
            print("ä½¿ç”¨éšæœºæ•°æ®è¿›è¡Œæµ‹è¯•")
    else:
        # ä½¿ç”¨éšæœºæ•°æ®
        data = torch.randn(2, 1, 128, 128)
        print("ä½¿ç”¨éšæœºæ•°æ®è¿›è¡Œæµ‹è¯•")
    
    data = data.to(device)
    
    # æ¨¡æ‹Ÿè®­ç»ƒæ­¥éª¤
    model.train()
    optimizer.zero_grad()
    
    # å‰å‘ä¼ æ’­
    reconstructed, latent = model(data)
    
    # è®¡ç®—æŸå¤±
    loss = criterion(reconstructed, data)
    
    # åå‘ä¼ æ’­
    loss.backward()
    optimizer.step()
    
    print(f"è®­ç»ƒæŸå¤±: {loss.item():.6f}")
    print(f"æ¢¯åº¦èŒƒæ•°: {torch.nn.utils.clip_grad_norm_(model.parameters(), float('inf')):.6f}")
    
    print("âœ… è®­ç»ƒæ­¥éª¤æµ‹è¯•é€šè¿‡!")
    return True


def create_dummy_data(output_dir: str = "./dummy_data", num_images: int = 10):
    """åˆ›å»ºè™šæ‹Ÿæ•°æ®ç”¨äºæµ‹è¯•"""
    print("=" * 50)
    print(f"åˆ›å»ºè™šæ‹Ÿæ•°æ®åˆ° {output_dir}...")
    
    output_path = Path(output_dir)
    output_path.mkdir(parents=True, exist_ok=True)
    
    from PIL import Image
    
    for i in range(num_images):
        # åˆ›å»ºéšæœºçš„uint16æ·±åº¦å›¾
        # æ¨¡æ‹Ÿæ·±åº¦å›¾çš„ç‰¹ç‚¹ï¼šå¤§éƒ¨åˆ†åŒºåŸŸæœ‰å€¼ï¼ŒæŸäº›åŒºåŸŸä¸º0
        height, width = 128, 128
        
        # åˆ›å»ºåŸºç¡€æ·±åº¦å›¾
        depth = np.random.randint(1000, 50000, (height, width), dtype=np.uint16)
        
        # æ·»åŠ ä¸€äº›æ— æ•ˆåŒºåŸŸï¼ˆå€¼ä¸º0ï¼‰
        mask = np.random.random((height, width)) > 0.2
        depth = depth * mask.astype(np.uint16)
        
        # æ·»åŠ ä¸€äº›å½¢çŠ¶ç‰¹å¾
        center_x, center_y = width // 2, height // 2
        y, x = np.ogrid[:height, :width]
        circle_mask = (x - center_x) ** 2 + (y - center_y) ** 2 < (30 + i * 2) ** 2
        depth[circle_mask] = depth[circle_mask] + 10000
        
        # ä¿å­˜ä¸ºPNGæ–‡ä»¶
        image = Image.fromarray(depth, mode='I;16')
        image.save(output_path / f"dummy_depth_{i:03d}.png")
    
    print(f"âœ… åˆ›å»ºäº† {num_images} å¼ è™šæ‹Ÿæ·±åº¦å›¾")
    return str(output_path)


def main():
    parser = argparse.ArgumentParser(description='AutoEncoderå¿«é€Ÿå¼€å§‹æµ‹è¯•')
    parser.add_argument('--data_dir', type=str, help='æ•°æ®ç›®å½•è·¯å¾„')
    parser.add_argument('--create_dummy', action='store_true', help='åˆ›å»ºè™šæ‹Ÿæ•°æ®')
    parser.add_argument('--dummy_dir', type=str, default='./dummy_data', help='è™šæ‹Ÿæ•°æ®è¾“å‡ºç›®å½•')
    parser.add_argument('--num_dummy', type=int, default=10, help='è™šæ‹Ÿæ•°æ®æ•°é‡')
    
    args = parser.parse_args()
    
    print("ğŸš€ AutoEncoder å¿«é€Ÿå¼€å§‹æµ‹è¯•")
    print("=" * 50)
    
    # åˆ›å»ºè™šæ‹Ÿæ•°æ®
    if args.create_dummy:
        dummy_data_dir = create_dummy_data(args.dummy_dir, args.num_dummy)
        if not args.data_dir:
            args.data_dir = dummy_data_dir
    
    # è¿è¡Œæµ‹è¯•
    tests_passed = 0
    total_tests = 3
    
    # æµ‹è¯•æ¨¡å‹
    if test_model():
        tests_passed += 1
    
    # æµ‹è¯•æ•°æ®é›†
    if test_dataset(args.data_dir):
        tests_passed += 1
    
    # æµ‹è¯•è®­ç»ƒæ­¥éª¤
    if test_training_step(args.data_dir):
        tests_passed += 1
    
    # æ€»ç»“
    print("=" * 50)
    print(f"æµ‹è¯•ç»“æœ: {tests_passed}/{total_tests} é€šè¿‡")
    
    if tests_passed == total_tests:
        print("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼å¯ä»¥å¼€å§‹æ­£å¼è®­ç»ƒäº†")
        print("\nä¸‹ä¸€æ­¥:")
        if args.data_dir:
            print(f"python train.py --data_dir {args.data_dir} --batch_size 8 --num_epochs 10")
        else:
            print("python train.py --data_dir /path/to/your/data --batch_size 32 --num_epochs 100")
    else:
        print("âŒ éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥é…ç½®")
    
    print("=" * 50)


if __name__ == '__main__':
    main() 