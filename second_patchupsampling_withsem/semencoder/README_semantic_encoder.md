# è¯­ä¹‰ç¼–ç å™¨å·¥å…·åŒ…

æœ¬å·¥å…·åŒ…æä¾›äº†ä»è®­ç»ƒå¥½çš„AutoEncoderä¸­æå–Encoderéƒ¨åˆ†ï¼Œå¹¶åˆ›å»ºç‹¬ç«‹å¯ç”¨çš„è¯­ä¹‰ç¼–ç å™¨çš„å®Œæ•´è§£å†³æ–¹æ¡ˆã€‚

## ğŸ“ æ–‡ä»¶ç»“æ„

```
â”œâ”€â”€ extract_encoder.py      # ä»AutoEncoderä¸­æå–Encoderæƒé‡
â”œâ”€â”€ semantic_encoder.py     # ç‹¬ç«‹çš„è¯­ä¹‰ç¼–ç å™¨æ¥å£
â”œâ”€â”€ usage_example.py        # å®Œæ•´ä½¿ç”¨ç¤ºä¾‹
â”œâ”€â”€ README_semantic_encoder.md  # æœ¬æ–‡æ¡£
â”œâ”€â”€ model.py               # åŸå§‹æ¨¡å‹å®šä¹‰
â””â”€â”€ train.py              # åŸå§‹è®­ç»ƒè„šæœ¬
```

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. æå–Encoderæƒé‡

ä»è®­ç»ƒå¥½çš„AutoEncoderæ£€æŸ¥ç‚¹ä¸­æå–Encoderéƒ¨åˆ†ï¼š

```bash
# åŸºæœ¬ä½¿ç”¨
python extract_encoder.py path/to/autoencoder_checkpoint.pth

# æŒ‡å®šè¾“å‡ºè·¯å¾„
python extract_encoder.py path/to/autoencoder_checkpoint.pth --output encoder_weights.pth

# éªŒè¯æå–çš„æƒé‡
python extract_encoder.py path/to/autoencoder_checkpoint.pth --verify
```

### 2. ä½¿ç”¨è¯­ä¹‰ç¼–ç å™¨

```python
from semantic_encoder import SemanticEncoder, create_semantic_encoder

# æ–¹æ³•1: ç›´æ¥åˆ›å»º
encoder = SemanticEncoder(
    input_channels=1,
    latent_dim=768,
    pretrained_path="encoder_weights.pth",
    freeze_weights=False
)

# æ–¹æ³•2: ä½¿ç”¨ä¾¿æ·å‡½æ•°
encoder = create_semantic_encoder(
    pretrained_path="encoder_weights.pth",
    freeze_weights=True  # ç”¨äºç‰¹å¾æå–
)

# æ¨ç†
import torch
x = torch.randn(4, 1, 128, 128)  # [B, C, H, W]
embeddings = encoder.encode(x)   # [B, 768]
```

## ğŸ“š è¯¦ç»†åŠŸèƒ½

### extract_encoder.py

**åŠŸèƒ½**ï¼šä»å®Œæ•´çš„AutoEncoderæ£€æŸ¥ç‚¹ä¸­æå–Encoderæƒé‡

**å‚æ•°**ï¼š
- `checkpoint_path`: AutoEncoderæ£€æŸ¥ç‚¹æ–‡ä»¶è·¯å¾„
- `--output`: è¾“å‡ºçš„encoderæƒé‡æ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼‰
- `--input_channels`: è¾“å…¥é€šé“æ•°ï¼ˆé»˜è®¤ï¼š1ï¼‰
- `--latent_dim`: æ½œåœ¨å‘é‡ç»´åº¦ï¼ˆé»˜è®¤ï¼š768ï¼‰
- `--verify`: éªŒè¯æå–çš„æƒé‡

**è¾“å‡ºæ ¼å¼**ï¼š
```python
{
    'encoder_state_dict': dict,  # Encoderæƒé‡
    'model_config': {
        'input_channels': int,
        'latent_dim': int,
        'model_type': str
    },
    'extracted_from': str,  # åŸå§‹æ–‡ä»¶è·¯å¾„
    'extraction_info': {
        'total_params': int,
        'trainable_params': int,
        'num_weights': int
    }
}
```

### semantic_encoder.py

**ä¸»è¦ç±»**ï¼š`SemanticEncoder`

**åˆå§‹åŒ–å‚æ•°**ï¼š
- `input_channels`: è¾“å…¥é€šé“æ•°ï¼ˆé»˜è®¤ï¼š1ï¼‰
- `latent_dim`: æ½œåœ¨å‘é‡ç»´åº¦ï¼ˆé»˜è®¤ï¼š768ï¼‰
- `pretrained_path`: é¢„è®­ç»ƒæƒé‡æ–‡ä»¶è·¯å¾„
- `freeze_weights`: æ˜¯å¦å†»ç»“æ‰€æœ‰æƒé‡
- `device`: è¿è¡Œè®¾å¤‡

**ä¸»è¦æ–¹æ³•**ï¼š

#### æƒé‡ç®¡ç†
```python
# åŠ è½½é¢„è®­ç»ƒæƒé‡
encoder.load_pretrained_weights("path/to/weights.pth")

# å†»ç»“/è§£å†»æ‰€æœ‰å‚æ•°
encoder.freeze_parameters(True)  # å†»ç»“
encoder.freeze_parameters(False) # è§£å†»

# å†»ç»“ç‰¹å®šå±‚
encoder.freeze_layers(['stem', 'layer1', 'layer2'])

# ä¿å­˜æƒé‡
encoder.save_encoder_weights("output_weights.pth")
```

#### æ¨ç†æ–¹æ³•
```python
# PyTorch Tensoræ¨ç†
embeddings = encoder.encode(tensor_input)

# NumPyæ•°ç»„æ¨ç†ï¼ˆè‡ªåŠ¨å¤„ç†ç»´åº¦å’Œå½’ä¸€åŒ–ï¼‰
embeddings = encoder.encode_numpy(numpy_input, normalize=True)

# å¤§æ‰¹é‡æ¨ç†ï¼ˆè‡ªåŠ¨åˆ†æ‰¹ï¼‰
embeddings = encoder.encode_batch(large_tensor, batch_size=16)
```

#### ä¿¡æ¯æŸ¥è¯¢
```python
# å‚æ•°æ•°é‡
total_params = encoder.count_parameters()
trainable_params = encoder.count_parameters(only_trainable=True)

# æ¨¡å‹ä¿¡æ¯
info = encoder.get_model_info()
print(encoder)  # è¯¦ç»†ä¿¡æ¯
```

## ğŸ”§ ä½¿ç”¨åœºæ™¯

### 1. ç‰¹å¾æå–å™¨

```python
# å†»ç»“æƒé‡ï¼Œç”¨ä½œç‰¹å¾æå–å™¨
feature_extractor = SemanticEncoder(
    pretrained_path="encoder_weights.pth",
    freeze_weights=True
)

# æ‰¹é‡æå–ç‰¹å¾
images = torch.randn(100, 1, 128, 128)
features = feature_extractor.encode_batch(images, batch_size=16)
```

### 2. ä¸‹æ¸¸ä»»åŠ¡çš„Backbone

```python
class MyModel(torch.nn.Module):
    def __init__(self):
        super().__init__()
        # åŠ è½½é¢„è®­ç»ƒencoder
        self.backbone = SemanticEncoder(
            pretrained_path="encoder_weights.pth",
            freeze_weights=False
        )
        # å†»ç»“æ—©æœŸå±‚
        self.backbone.freeze_layers(['stem', 'layer1'])
        
        # ä»»åŠ¡ç‰¹å®šçš„å¤´éƒ¨
        self.classifier = torch.nn.Linear(768, num_classes)
    
    def forward(self, x):
        features = self.backbone.encode(x)
        return self.classifier(features)
```

### 3. ç›¸ä¼¼åº¦è®¡ç®—

```python
encoder = SemanticEncoder(pretrained_path="encoder_weights.pth")

# è®¡ç®—ä¸¤ä¸ªå›¾åƒçš„ç›¸ä¼¼åº¦
with torch.no_grad():
    feat1 = encoder.encode(img1)
    feat2 = encoder.encode(img2)
    similarity = torch.nn.functional.cosine_similarity(feat1, feat2)
```

### 4. æ•°æ®å¤„ç†æµæ°´çº¿

```python
import numpy as np

# å¤„ç†NumPyæ ¼å¼çš„æ•°æ®
def process_depth_maps(depth_maps: np.ndarray) -> np.ndarray:
    """
    å¤„ç†æ·±åº¦å›¾æ•°ç»„
    Args:
        depth_maps: [N, H, W] æˆ– [N, C, H, W]
    Returns:
        embeddings: [N, 768]
    """
    encoder = SemanticEncoder(
        pretrained_path="encoder_weights.pth",
        freeze_weights=True
    )
    
    return encoder.encode_numpy(
        depth_maps, 
        batch_size=32,
        normalize=True
    )
```

## âš™ï¸ é…ç½®å‚æ•°

### æ¨¡å‹é…ç½®
- **input_channels**: è¾“å…¥é€šé“æ•°
  - `1`: æ·±åº¦å›¾/ç°åº¦å›¾
  - `3`: RGBå›¾åƒ
- **latent_dim**: æ½œåœ¨å‘é‡ç»´åº¦
  - é»˜è®¤ï¼š`768`ï¼ˆä¸è®­ç»ƒæ—¶ä¿æŒä¸€è‡´ï¼‰

### è®¾å¤‡é…ç½®
- **device**: è¿è¡Œè®¾å¤‡
  - `"cuda"`: GPU
  - `"cpu"`: CPU
  - `None`: è‡ªåŠ¨é€‰æ‹©

### æƒé‡å†»ç»“ç­–ç•¥
- **freeze_weights**: å…¨å±€å†»ç»“å¼€å…³
- **freeze_layers**: é€‰æ‹©æ€§å†»ç»“ç‰¹å®šå±‚
  - å¸¸ç”¨å±‚åï¼š`["stem", "layer1", "layer2", "layer3", "layer4"]`

## ğŸ” è°ƒè¯•ä¸éªŒè¯

### æƒé‡éªŒè¯
```python
# æ£€æŸ¥æƒé‡åŠ è½½æ˜¯å¦æˆåŠŸ
encoder = SemanticEncoder(pretrained_path="weights.pth")
test_input = torch.randn(1, 1, 128, 128)
output = encoder.encode(test_input)
print(f"è¾“å‡ºå½¢çŠ¶: {output.shape}")  # åº”è¯¥æ˜¯ [1, 768]
```

### å‚æ•°æ£€æŸ¥
```python
# æŸ¥çœ‹å‚æ•°çŠ¶æ€
info = encoder.get_model_info()
print(f"æ€»å‚æ•°: {info['total_parameters']:,}")
print(f"å¯è®­ç»ƒå‚æ•°: {info['trainable_parameters']:,}")

# æ£€æŸ¥æ¢¯åº¦çŠ¶æ€
for name, param in encoder.named_parameters():
    print(f"{name}: requires_grad={param.requires_grad}")
```

### æ€§èƒ½æµ‹è¯•
```python
import time

# æµ‹è¯•æ¨ç†é€Ÿåº¦
encoder.eval()
test_batch = torch.randn(32, 1, 128, 128)

with torch.no_grad():
    start_time = time.time()
    output = encoder.encode(test_batch)
    end_time = time.time()

print(f"æ‰¹é‡æ¨ç†æ—¶é—´: {(end_time - start_time)*1000:.2f} ms")
print(f"å•å¼ å›¾åƒæ¨ç†æ—¶é—´: {(end_time - start_time)*1000/32:.2f} ms")
```

## ğŸ“‹ å®Œæ•´ç¤ºä¾‹

å‚è§ `usage_example.py` æ–‡ä»¶ï¼ŒåŒ…å«äº†æ‰€æœ‰åŠŸèƒ½çš„è¯¦ç»†ç¤ºä¾‹ï¼š

```bash
python usage_example.py
```

ç¤ºä¾‹åŒ…æ‹¬ï¼š
1. ä»AutoEncoderæå–æƒé‡
2. åŸºæœ¬ä½¿ç”¨æ–¹æ³•
3. æ¨ç†ä½¿ç”¨
4. å‚æ•°æ§åˆ¶
5. å®é™…åº”ç”¨åœºæ™¯
6. æ¨¡å‹ä¿å­˜å’ŒåŠ è½½

## ğŸ› å¸¸è§é—®é¢˜

### Q1: æƒé‡åŠ è½½å¤±è´¥
**é—®é¢˜**ï¼š`æƒé‡æ–‡ä»¶ä¸å­˜åœ¨` æˆ– `æœªæ‰¾åˆ°encoderæƒé‡`

**è§£å†³**ï¼š
1. æ£€æŸ¥æ–‡ä»¶è·¯å¾„æ˜¯å¦æ­£ç¡®
2. ç¡®ä¿ä½¿ç”¨ `extract_encoder.py` æ­£ç¡®æå–äº†æƒé‡
3. æ£€æŸ¥åŸå§‹AutoEncoderæ£€æŸ¥ç‚¹æ˜¯å¦å®Œæ•´

### Q2: è¾“å…¥ç»´åº¦ä¸åŒ¹é…
**é—®é¢˜**ï¼š`Expected 4D tensor, got 3D tensor`

**è§£å†³**ï¼š
```python
# ç¡®ä¿è¾“å…¥æ˜¯4Då¼ é‡ [B, C, H, W]
if input_tensor.ndim == 3:
    input_tensor = input_tensor.unsqueeze(1)  # æ·»åŠ é€šé“ç»´åº¦
```

### Q3: å†…å­˜ä¸è¶³
**é—®é¢˜**ï¼šGPUå†…å­˜ä¸è¶³

**è§£å†³**ï¼š
```python
# ä½¿ç”¨æ‰¹é‡æ¨ç†
embeddings = encoder.encode_batch(large_input, batch_size=16)

# æˆ–è€…ä½¿ç”¨CPU
encoder = SemanticEncoder(device='cpu')
```

### Q4: å‚æ•°å†»ç»“ä¸ç”Ÿæ•ˆ
**é—®é¢˜**ï¼šå‚æ•°ä»ç„¶åœ¨æ›´æ–°

**è§£å†³**ï¼š
```python
# æ£€æŸ¥å‚æ•°çŠ¶æ€
for name, param in encoder.named_parameters():
    if param.requires_grad:
        print(f"æœªå†»ç»“çš„å‚æ•°: {name}")

# å¼ºåˆ¶å†»ç»“
encoder.freeze_parameters(True)
```

## ğŸ“ æ³¨æ„äº‹é¡¹

1. **ç‰ˆæœ¬å…¼å®¹æ€§**ï¼šç¡®ä¿PyTorchç‰ˆæœ¬ä¸è®­ç»ƒæ—¶ä¸€è‡´
2. **è®¾å¤‡ä¸€è‡´æ€§**ï¼šæƒé‡æ–‡ä»¶å’Œæ¨¡å‹éœ€è¦åœ¨åŒä¸€è®¾å¤‡ä¸Š
3. **è¾“å…¥æ ¼å¼**ï¼šç¡®ä¿è¾“å…¥æ•°æ®æ ¼å¼ä¸è®­ç»ƒæ—¶ä¸€è‡´ï¼ˆå½’ä¸€åŒ–èŒƒå›´ç­‰ï¼‰
4. **å†…å­˜ç®¡ç†**ï¼šå¤§æ‰¹é‡æ¨ç†æ—¶æ³¨æ„å†…å­˜ä½¿ç”¨
5. **æ¢¯åº¦è®¡ç®—**ï¼šæ¨ç†æ—¶è®°å¾—ä½¿ç”¨ `torch.no_grad()`

## ğŸ”„ æ›´æ–°æ—¥å¿—

- **v1.0**: åŸºç¡€åŠŸèƒ½å®ç°
  - æƒé‡æå–
  - åŸºæœ¬æ¨ç†æ¥å£
  - å‚æ•°å†»ç»“æ§åˆ¶
  - NumPyå…¼å®¹æ€§

## ğŸ“ æ”¯æŒ

å¦‚æœ‰é—®é¢˜ï¼Œè¯·æ£€æŸ¥ï¼š
1. è¿è¡Œ `usage_example.py` ç¡®è®¤åŸºæœ¬åŠŸèƒ½
2. æŸ¥çœ‹é”™è¯¯ä¿¡æ¯å’Œå †æ ˆè·Ÿè¸ª
3. æ£€æŸ¥è¾“å…¥æ•°æ®æ ¼å¼å’Œç»´åº¦ 